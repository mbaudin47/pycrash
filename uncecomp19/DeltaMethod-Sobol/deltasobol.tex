% Michaël Baudin, 2017
\documentclass{article}

\include{macros}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{The Delta-Method applied to Sobol' indices}

\author{Michaël Baudin}

\maketitle


\abstract{
We explore the use of the Delta-method in order to estimate 
the Sobol' indices.
}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Convergence in distribution}

\begin{definition}
\label{def-convdistr}
(\emph{Convergence in distribution})
Assume that $X_1,X_2,...$ is a sequence of real-valued random variables 
with cumulative distribution functions $\{F_n\}_{n\geq 0}$. 
Assume that $X$ is a real-valued random variable 
with cumulative distribution function $F$. 
The sequence $X_n$ converges in distribution to $X$ if:
$$
\lim_{n\rightarrow \infty} F_n(X_n)=F(x).
$$
for any $x\in \RR$ at which $F$ is continuous.
In this case, we write:
$$
X_n \xrightarrow{D} X.
$$
\end{definition}

The following theorem gives an example of such convergence. 

\begin{example}
(\emph{Maximum of uniform random numbers})
Assume that $X_1,X_2,...$ are independent uniform random numbers such that $X_n\sim \mathcal{U} (0,1)$. 
Let $Y_n$ be the maximum:
$$
Y_n = \max_{1\leq i\leq n}  X_i.
$$
Therefore the sequence $n(1-Y_n)$ converges in distribution to an exponential random variable, i.e.:
$$
n(1-Y_n) \xrightarrow{D} \mathcal{E}(1).
$$
\end{example}

\begin{proof}
By definition, the exponential distribution with rate $\lambda$ has the cumulative distribution function:
$$
F(y) = 1 - \exp(-\lambda y),
$$
for any real $y\geq 0$.
We apply the previous equality to $\lambda=1$ which shows that we must prove that:
$$
F(y) = 1 - \exp(-y),
$$
for any real $y\geq 0$. 
Let $F_n$ be the cumulative distribution function of the random variable $n(1-Y_n)$. 
By definition of the cumulative distribution function, 
\begin{align*}
F_n(z) 
&= P(n(1-Y_n)\leq z) \\
&= P(1-Y_n\leq z/n) \\
&= P(1-z/n\leq Y_n) \\
&= 1-P(Y_n\leq 1-z/n),
\end{align*}
for any $z\in\RR$. 
However, the cumulative distribution function of the maximum is: 
\begin{align*}
P(Y_n\leq y)
& = P(X_1\leq y, X_2\leq y,..., X_n\leq y) \\
& = P(X_1\leq y)P(X_2\leq y) \cdots P(X_n\leq y) \textrm{ (by independence)}\\
& = y y \cdots y \textrm{ (by definition of the C.D.F. of a uniform random variable)}\\
& = y^n,
\end{align*}
for any $y\in[0,1]$. 
Therefore, 
\begin{align*}
F_n(z) 
&= 1-(1-z/n)^n
\end{align*}
for any $z\in\RR$. 
But 
\begin{align*}
\lim_{n\rightarrow \infty} (1+z/n)^n = \exp(z)
\end{align*}
for any $z\in\RR$. 
Therefore,
\begin{align*}
\lim_{n\rightarrow\infty} F_n(z) 
&= 1-\exp(-z)
\end{align*}
for any $z\in\RR$ which concludes the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Convergence in probability}

\begin{definition}
(\emph{Convergence in probability})
Assume that $X_1,X_2,...$ is a sequence of real-valued random variables. 
The sequence $X_n$ converges in probability to $X$ if, for any $\epsilon>0$:
$$
\lim_{n\rightarrow \infty} P(|X_n-X|>\epsilon)=0.
$$
In this case, we write:
$$
X_n \xrightarrow{P} X.
$$
\end{definition}

\begin{example}
(\emph{Convergence of an exponential random variable})
Assume that $X_1,X_2,...$ are independent random numbers such that $X_n\sim \mathcal{E}xp(n)$. 
Therefore the sequence $X_n$ converges in probability to the zero random variable:
$$
X_n \xrightarrow{P} 0.
$$
\end{example}

\begin{proof}
By definition of the exponential cumulative distribution function, we have 
$$
P(X_n\leq x) = 1 - \exp(-nx),
$$
for any real number $x\geq 0$. 
Let $X=0$. 
For any $\epsilon>0$, we have:
\begin{align*}
P(|X_n-X|>\epsilon)
&= P(|X_n|>\epsilon) \textrm{ (since $X=0$)} \\
&= P(X_n>\epsilon) \textrm{ (since $X_n\geq 0$)} \\
&= 1 - P(X_n\leq \epsilon) \\
&= \exp(-n\epsilon)
\end{align*}
by definition of the exponential cumulative distribution function. 
Since $\epsilon>0$, this implies:
\begin{align*}
\lim_{n\rightarrow \infty} \exp(-n\epsilon) = 0 
\end{align*}
which implies that 
\begin{align*}
\lim_{n\rightarrow \infty} P(|X_n-X|>\epsilon) = 0 
\end{align*}
and concludes the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Convergence in probability implies convergence in distribution}

\begin{theorem}
\label{theo-doubleproba}
Assume that $X$ and $Y$ are random variables. 
Let $y\in\RR$ and let $\epsilon>0$. 
Therefore
\begin{align}
\label{eq-doubleproba}
P(Y\leq y) \leq P(X\leq y+\epsilon) + P(|Y-X|> \epsilon)
\end{align}
\end{theorem}

\begin{proof}
\begin{align*}
P(Y\leq y)
& = P(Y\leq y,X\leq y+\epsilon)+P(Y\leq y,X> y+\epsilon).
\end{align*}
But $P(Y\leq y,X\leq y+\epsilon) \leq P(X\leq y+\epsilon)$, which implies :
\begin{align*}
P(Y\leq y)
& \leq P(X\leq y+\epsilon)+P(Y\leq y,X> y+\epsilon) \\
& \leq P(X\leq y+\epsilon)+P(Y-X\leq y-X, y+\epsilon<X) \\
& \leq P(X\leq y+\epsilon)+P(Y-X\leq y-X, y-X <-\epsilon).
\end{align*}
Moreover, we have $P(Y-X\leq y-X, y-X <-\epsilon)\leq P(Y-X<-\epsilon)$ which implies : 
\begin{align*}
P(Y\leq y)
& \leq P(X\leq y+\epsilon)+P(Y-X<-\epsilon).
\end{align*}
By definition of a probability, we have $P(Y-X>\epsilon)\geq 0$. 
Therefore, 
\begin{align*}
P(Y\leq y)
& \leq P(X\leq y+\epsilon)+P(Y-X<-\epsilon) + P(Y-X>\epsilon)
\end{align*}
which leads to the equation \ref{eq-doubleproba} and concludes the proof.
\end{proof}


\begin{theorem}
\label{theo-convprovdist}
(\emph{Convergence in probability implies convergence in distribution})
Assume that $X_1,X_2,...$ is a sequence of real-valued random variables. 
If 
$$
X_n \xrightarrow{P} X
$$
then
$$
X_n \xrightarrow{D} X.
$$
\end{theorem}

\begin{proof}
Assume that $\{F_n\}_{n\geq 0}$ are the cumulative distribution functions of $X_1,X_2,...$. 
Assume that $X$ is a real-valued random variable 
with cumulative distribution function $F$ and that 
$X_n \xrightarrow{P} X$. 
Let $x\in \RR$ be a point at which $F$ is continuous.
By the definition \ref{def-convdistr} we must prove that 
\begin{align}
\label{eq-convprovdist1}
\lim_{n\rightarrow \infty} F_n(X_n)=F(x).
\end{align}

Let $\epsilon>0$. 

First, we first apply the theorem \ref{theo-doubleproba} to the random variables $X_n$ and $X$. 
The equation \ref{eq-doubleproba} implies:
\begin{align*}
P(X_n\leq x) \leq P(X\leq x+\epsilon) + P(|X_n-X|> \epsilon).
\end{align*}
By hypothesis, the sequence $X_1,X_2,...$ converges in probability to $X$, which 
implies:
\begin{align*}
\lim_{n\rightarrow \infty} P(X_n\leq x) \leq P(X\leq x+\epsilon).
\end{align*}
since 
$\lim_{n\rightarrow \infty} P(|X_n-X|> \epsilon) = 0$. 
By definition of the cumulatif distribution function $F$, this implies:
\begin{align}
\label{eq-convprovdist2}
\lim_{n\rightarrow \infty} P(X_n\leq x) \leq F(x+\epsilon).
\end{align}

Secondly, we consider the theorem \ref{theo-doubleproba} and derive a similar  
inequality. 
In the equation \ref{eq-doubleproba}, we exchange the random variables $X$ and $Y$:
\begin{align*}
P(X\leq y) \leq P(Y\leq y+\epsilon) + P(|X-Y|> \epsilon)
\end{align*}
and we apply the inequality to the real $y-\epsilon$; this leads to:
\begin{align*}
P(X\leq y-\epsilon) \leq P(Y\leq y) + P(|X-Y|> \epsilon).
\end{align*}

Back to the theorem \ref{theo-convprovdist}, we apply the previous inequality 
to the random variables $X$ and $X_n$, at the point $x$. 
We obtain:
\begin{align*}
P(X\leq x-\epsilon) \leq P(X_n\leq x) + P(|X-X_n|> \epsilon).
\end{align*}
By hypothesis, the sequence $X_1,X_2,...$ converges in probability to $X$, which 
implies:
\begin{align*}
P(X\leq x-\epsilon) \leq \lim_{n\rightarrow \infty} P(X_n\leq x) .
\end{align*}
By definition of the cumulatif distribution function $F$, this implies:
\begin{align}
\label{eq-convprovdist3}
F(x-\epsilon) \leq \lim_{n\rightarrow \infty} P(X_n\leq x) .
\end{align}

Combining the inequalities \ref{eq-convprovdist2} and \ref{eq-convprovdist3}, we obtain: 
\begin{align*}
F(x-\epsilon) \leq \lim_{n\rightarrow \infty} P(X_n\leq x)  \leq F(x+\epsilon).
\end{align*}
The function $F$ is, by hypothesis, continuous at the point $x$ which implies than we can take 
the limit of the previous inequality when $\epsilon\rightarrow 0$. 
This implies:
\begin{align*}
F(x) \leq \lim_{n\rightarrow \infty} P(X_n\leq x)  \leq F(x)
\end{align*}
which leads to the equation \ref{eq-convprovdist1} and concludes the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Delta method}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}
\label{theo-jointrandomvector}
(\emph{Joint random vector convergence in distribution})
Let $X_n$ and $Y_n$ be two sequences of real random variables such that:
$$
X_n \xrightarrow{D} X \quad \textrm{ and } \quad Y_n \xrightarrow{P} c,
$$
where $c$ is a non-random constant. 
Therefore, the joint random vector $(X_n,Y_n)$ converges in distribution to $(X,c)$:
$$
(X_n,Y_n) \xrightarrow{D} (X, c) .
$$
\end{theorem}

\begin{proof}
See \cite{Vaart2000}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}
\label{theo-continuousmap}
(\emph{Continuous mapping})
Let $X_n$ be a sequence of real random variables such that:
$$
X_n \xrightarrow{D} X.
$$
Assume that $g$ is a continous function on $\RR$. 
Therefore:
$$
g(X_n) \xrightarrow{D} g(X).
$$
\end{theorem}

\begin{proof}
See \cite{Vaart2000}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}
(\emph{Slutky's theorem})
Let $X_n$ and $Y_n$ be two sequences of real random variables such that:
$$
X_n \xrightarrow{D} X \quad \textrm{ and } \quad Y_n \xrightarrow{P} c,
$$
where $c$ is a non-random constant. 
Therefore, 
\begin{enumerate}
\item $X_n Y_n \xrightarrow{D} X c  $,
\item $X_n + Y_n \xrightarrow{D} X + c$,
\item $X_n / Y_n \xrightarrow{D} X / c$.
\end{enumerate}
\end{theorem}

\begin{proof}
Let us prove that  $X_n Y_n \xrightarrow{D} X c$. 
Let $g$ be the continuous function defined by $g(x,y)=x y$, for any $x,y\in\RR$. 
By the theorem \ref{theo-jointrandomvector}, we have $(X_n,Y_n) \xrightarrow{D} (X, c)$. 
By the continuous mapping theorem, this implies:
$$
X_n Y_n = g((X_n,Y_n)) \xrightarrow{D} g((X,c)) = X  c. 
$$

In order to prove the equation $X_n + Y_n \xrightarrow{D} X + c$, we use the 
continous fonction $g(x,y)=x + y$, which immediately leads to the required result. 

Finally, let $g$ be the continuous function defined by $g(x,y)=x / y$, for any $x,y\in\RR$ such that $y\neq 0$. 
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}
(\emph{Univariate delta-method})
Assume that $X_1,X_2,...\in\RR$ is a sequence of real-valued random variables 
and $\theta\in\RR$ are so that 
\begin{align}
\label{eq-delmeth1}
\sqrt{n} (X_n - \theta) \xrightarrow{D} \mathcal{N}(0,\sigma^2).
\end{align}
Assume that $g\in C^1(\RR)$ and that $g'(\theta)\neq 0$. 
Therefore, 
$$
\sqrt{n} (g(X_n) - g(\theta)) \xrightarrow{D} \mathcal{N}(0,\sigma^2 g'(\theta)^2).
$$
\end{theorem}

\begin{proof}
By hypothesis, we have $g\in C^1(\RR)$ so that we can apply Taylor's theorem. 
Taylor's expansion of $g$ at the point $\theta$ implies that there exists a $\tilde{\theta}$ between 
$X_n$ and $\theta$ such that:
$$
g(X_n) = g(\theta) + g'\left(\tilde{\theta}\right)(X_n-\theta).
$$
This implies:
\begin{align}
\label{eq-delmeth2}
\sqrt{n} (g(X_n) - g(\theta)) = g'\left(\tilde{\theta}\right) \sqrt{n} (X_n-\theta).
\end{align}

Since $\tilde{\theta}$ is between $X_n$ and $\theta$, for any $\epsilon>0$, we have:
$$
\lim_{n\rightarrow \infty} P(|\tilde{\theta} - \theta|>\epsilon)=0.
$$
This implies:
$$
\tilde{\theta} \xrightarrow{P} \theta.
$$
By hypothesis, the function $g$ is continously differentiable, which implies that $g'$ is a 
continuous function. 
Therefore, the continuous mapping theorem implies:
$$
g'\left(\tilde{\theta}\right) \xrightarrow{P} g'(\theta).
$$

The equation \ref{eq-delmeth1} and Slutky's theorem imply:
$$
g'\left(\tilde{\theta}\right) \sqrt{n} (X_n-\theta) 
\xrightarrow{D} g'(\theta) \mathcal{N}(0,\sigma^2) = \mathcal{N}(0,\sigma^2 g'(\theta)^2),
$$
where the last equation is from the properties of the gaussian 
distribution. 
The equation \ref{eq-delmeth2} concludes the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}
\label{eq-multivdelmeth1}
(\emph{Multivariate delta-method})
Assume that $\bX_1,\bX_2,... \in\RR^p$ is a sequence of random variables 
and $\btheta\in\RR^p$ are so that 
\begin{align}
\label{eq-multivdelmeth1}
\sqrt{n} (\bX_n - \btheta) \xrightarrow{D} \mathcal{N}\left(\bzero,\Sigma\right),
\end{align}
where $\Sigma\in\RR^{p\times p}$ is a positive semi-definite covariance matrix. 
Suppose that $\bg:\RR^p \rightarrow \RR^m \in C^1(\RR^p)$. 
For any $\btheta\in\RR^p$, let $J(\btheta)\in\RR^{m\times p}$ be the Jacobian matrix of $\bg$:
$$
J(\btheta) = 
\begin{pmatrix}
\frac{\partial g_1}{\partial \theta_1} & 
\cdots &
\frac{\partial g_1}{\partial \theta_p} \\
\vdots &    & \vdots \\
\frac{\partial g_m}{\partial \theta_1} & 
\cdots &
\frac{\partial g_m}{\partial \theta_p} 
\end{pmatrix} 
$$
Assume that $\nabla \bg(\btheta)\neq \bzero$. 
Therefore, 
$$
\sqrt{n} (\bg(\bX_n) - \bg(\btheta)) \xrightarrow{D} \mathcal{N}\left(\bzero, J(\btheta)^T \Sigma J(\btheta)\right).
$$
\end{theorem}

In the previous theorem, notice that $\bzero\in\RR^p$. 

\begin{proof}
By hypothesis, we have $\bg\in C^1(\RR^p)$ so that we can apply Taylor's theorem. 
Taylor's expansion of the function $\bg$ at the point $\btheta$ implies that there exists a $\tilde{\btheta}\in \RR^p$ between 
$\bX_n$ and $\btheta$ such that:
$$
\bg(\bX_n) = \bg(\btheta) + J\left(\tilde{\btheta}\right) (\bX_n-\btheta).
$$
This implies:
\begin{align}
\label{eq-multivdelmeth2}
\sqrt{n} (\bg(\bX_n) - \bg(\btheta)) = J\left(\tilde{\btheta}\right) \sqrt{n} (\bX_n-\btheta).
\end{align}

By definition of the variance, 
\begin{align*}
V\left(\sqrt{n} (g(X_n) - g(\theta))\right) 
&= V\left(J\left(\tilde{\theta}\right) \sqrt{n} (X_n-\theta) \right) \\
\end{align*}

TODO : finish this.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{References}

Most of the results in this document can be found in \cite{Vaart2000}.

\bibliographystyle{plain}
\bibliography{deltasobol}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
